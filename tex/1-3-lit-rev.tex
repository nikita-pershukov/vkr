\section{Обзор литературы}
\label{sec:lit-rev}

В первую очередь интересны и полезны статьи от IBM~\cite{ibm:containers1}\cite{ibm:containers2}. Начинаются они со сравнения виртуальных машин и контейнеров, которое было приведено выше в разделе \ref{sec:history} видов архитектуры. Затем идет перечисление и детальное рассмотрение каждого из преимуществ использования контейнеров в архитектуре инфраструктуры.

Статья~\cite{habr:ruvds:jenkins-vs-gitlab} освещает сравнение двух популярнейших систем Continuous Integration и Continuous Delivery или Deployment (далее -- CI/CD, этапы непрерывной интеграции разработок и непрерывной доставки кода вплоть до промышленного использования)~\cite{habr:flant:k8s-and-gitlab}
\Abbrev{CI/CD}{Continuous Integration и Continuous Delivery или Deployment}
\Define{Continuous Integration и Continuous Delivery или Deployment}{этапы непрерывной интеграции разработок и непрерывной доставки кода вплоть до промышленного использования~\cite{habr:flant:k8s-and-gitlab}}
-- Jenkins и GitLab CI/CD. В ней рассмотрены основные особенности, функциональные возможности и преимущества двух систем. Затем дается достаточно объективное сравнение этих инструментов, что позволяет выбрать необходимый под создаваемую инфраструктуру.

По аналогии с сравнением систем CI/CD, стоит обратить внимание на сравнение систем оркестрации контейнеров~\cite{al2019container}. Среди рассматриваемых инструментов: Swarm, Kubernetes,
\Abbrev{K8s}{Kubernetes}
\Define{Kubernetes}{blablabla}
Mesos, Cattle. Как один из немногих недостатков Kubernetes можно отметить отсутствие ограничений на обращение к диску. Но в тот же момент, среди рассматриваемых систем этим преимуществом обладает только Mesos. В остальном же Kubernetes выигрывает по функциональности с большим отрывом. Если посмотреть на приведенные в публикации графики, то можно увидеть, что есть не один случай, когда Kubernetes выигрывает с большим отрывом от других средств. В других же случаях он на равных с большинством.

Если рассмотреть прикладные статьи по тематике контейнеризации, то внимания заслуживает следующий материал~\cite{nust2020ten}. В нем четко сформулированы простые правила написания инструкций для сборки Docker Images. Как аналогия приводится написание кода на языке программирования C или C++. Также полезны листинги с примерами файлов-инструкций.

В дополнение к прикладным статьям, стоит обратить внимание на еще одну~\cite{gruening2018recommendations}. В ней также рассмотрены преимущества контейнеризации, ее особенности. Из ключевых стоит отметить:
\begin{itemize}
    \item Простоту;
    \item Поддерживаемость;
    \item Устойчивость;
    \item Воспроизводимость;
    \item Удобство;
    \item Размер;
    \item Прозрачность.
\end{itemize}

Нельзя обойти стороной рассмотрение официальных лучших практик по контейнеризации от разработчиков самого Docker~\cite{docker:best-practicies}. Первым и, пожалуй, ключевым они выделяют важность порядка инструкций, потому что если что-то изменяется выше, то все последующие инструкции будут тоже исполнятся заново, что сильно увеличивает время сборки Docker Image.

В другой статье~\cite{wang2019developing} приводится сравнение вертикального и горизонтального масштабирования. Это давняя известная проблема: вместо наращивания мощности одного инстанса, например, сервера, лучше взять еще один такой же конфигурации. Это позволяет экономить финансы. Но опять же, приложение архитектурно должно быть к этому готово.

Если же рассмотреть промышленное использование в больших масштабах, то стоит обратить внимание на данную книгу~\cite{indrasiri2018microservices}. Она рассказывает о <<кровавом энтерпрайзе>>, что позволяет выявить ряд недостатков в выбранной архитектуре, так как не всегда она подходит под нужды бизнеса.

Для сравнения систем оркестрации можно изучить статью об одной из них и ее использовании~\cite{naik2016building}. С одной стороны, она тоже намного мощнее такого инструмента как docker-compose, который подходит для одного микросервиса. Но в сравнении с Kubernetes есть ряд очень ощутимых преимуществ у последнего.

Чтобы лучше понять логику <<пакетного менеджера для приложений>>\ -- Helm, стоит обратить внимание на статью о его истории и будущем~\cite{habr:flant:helm}. Как минимум, заслуживает внимание разбор гигантское обновление со второй на третью версию этого инструмента, где отказались от части логики, из-за чего отсутствует обратная совместимость.

Для рассмотрения связки GitLab и Kubernetes отлично подходит статья компании, которая помогает внедрять эти технологии огромному количеству малого и среднего бизнесов -- Flant~\cite{habr:flant:k8s-and-gitlab}. Более того, эта статья заслуживает внимания, так как является выдержкой с выступления на одной из крупнейших конференций в России по высоконагруженным системам -- Highload++ 2017. В первую очередь хочется отметить наглядность анимированного изображения, которое позволяет наглядно увидеть разницу между разными инструментами CI/CD.
Данное изображение дает понять, что:
\begin{itemize}
    \item Git (одна из систем контроля версий)~\cite{def:git}
        \Define{Git}{одна из систем контроля версий~\cite{def:git}}
        вместе с shell
        \Define{Shell}{средство для запуска других программ в ОС GNU/Linux}
        дает несколько окружений и анализ кода;
    \item Добавление Docker позволит улучшить тестирование до тестов без окружения и добавит аспект Stateless
        \Define{Stateless}{blablabla}
        приложения в архитектуру;
    \item Дополнительное использование Kubernetes и Helm позволяет довести тестирование до тестов в <<полном>> окружении, а архитектуру привести к микросервисной логике;
    \item При пополнении стека с помощью GitLab мы получаем несколько площадок вместо нескольких окружений, а также простое разделение прав доступа. Этого набора уже зачастую предостаточно для покрытия нужд;
    \item В заключение -- GitLab Enterprise, который привносит разные права на окружения, <<multi stage approval>>
        \Define{Multi stage approval}{blablabla}
        или же <<quorum approval>>
        \Define{Quorum approval}{blablabla}
        логики.
\end{itemize}

В итоге у компании Flant используется один из самых популярных стеков технологий, за исключением dapp, который на текущий момент переименован в werf, их личная разработка, которая используется для упрощения или улучшения процессов сборки и не только.

В материалах предыдущей статьи есть ссылки на другие публикации, что позволяет глубже погрузиться в тематику. Если посмотреть на обзор другого доклада с RootConf 2017~\cite{habr:flant:k8s-small-projects}, то можно заметить пересечения с вышеупомянутой историей развития архитектуры. При этом освещены вопросы, которые не обсуждались ранее, а именно ряд сложностей микросервисной архитектуры:
\begin{itemize}
    \item Сбор логов;
    \item Сбор метрик;
    \item Проверка состояния сервисов и их перезапуск в случае проблем;
    \item Автоматическое обнаружение сервисов;
    \item Автоматизация обновления конфигураций компонентов инфраструктуры (при добавлении/удалении новых сущностей сервисов);
    \item Масштабирование;
    \item CI/CD;
    \item Зависимость от выбранного <<поставщика решения>>.
\end{itemize}

По мнению докладчика, все эти сложности можно закрыть с помощью Kubernetes и ряда вспомогательных инструментов.

Далее в статье~\cite{habr:flant:k8s-small-projects} освещаются базовые аспекты архитектуры Kubernetes, его сущности от Pod до Ingress. Следующий раздел этой публикации погружает нас в разные архитектуры Kubernetes, в зависимости от нагрузки, требований к отказоустойчивости и других параметров, что еще раз демонстрирует его гибкость как инструмента.

Если изучить еще одну статью~\cite{habr:flant:cd-docker}, которая затрагивает первые практики Continuous Delivery с Docker, то можно раскрыть детальнее первый из этих терминов. Под Continuous Delivery имеется в виду последовательность действий, в результате который код из Git-репозитория сначала собирается, потом тестируется, после чего попадает в промышленное использование и после уходит в архив. Также в статье поднимается вопрос проблемы простоя во время выкатки в промышленное использование новой версии продукта. На текущий момент это решается следующей последовательностью:
\begin{itemize}
    \item Старая версия запущена;
    \item В соседнем месте запускается и “прогревается” новая версия;
    \item Когда новая версия готова к работе, трафик переключается на нее;
    \item Старая версия может быть остановлена.
\end{itemize}

Не меньшего внимания заслуживает еще одна статья от компании-интегратора Flant уже про базы данных в Kubernetes~\cite{habr:flant:db-and-k8s}. Основная проблематика заключается в том, что Kubernetes ориентирован на stateless приложения, то есть приложения без сохранения состояния. А БД
\Abbrev{БД}{база данных}
-- яркий пример stateful приложения,
\Define{Statefull application}{blablabla}
которому жизненно необходимо сохранение своего состояния. В старой архитектуре приложений была следующая логика с СУБД: репликация на двух железных серверах с резервированным питанием диском сетью и всем остальным, включая инженера на дежурной смене. Это позволяло гарантировать, что если что-то или кто-то выйдет из строя, то есть возможность оперативно переключиться или заменить неисправный элемент. В архитектуре же Kubernetes ощутимо другая логика, но она тоже привносит отказоустойчивость:
\begin{itemize}
    \item Логика кворума вместе с логикой логикой активного и запасного мастер-компонентов, которые управляют кластером;
    \item Автоматический <<переезд>> сущностей с упавшего сервера на остальные;
    \item Возможность декларативно описать логику поведения при недоступности сущности.
\end{itemize}

Как простое решение с низким показателем критичности в аспекте отказоустойчивости предлагается сущность StatefulSet с одной сущностью Pod, что означает запуск СУБД
\Abbrev{СУБД}{система управления базами данных}
или другого stateful приложения в одном экземпляре. Этого может быть вполне достаточно для тестовых сред. Чуть более сложная схема -- StatefulSet уже с двумя инстансами, между которыми настроена репликация данных с возможностью переключение с активного приложения на запасное. Но оптимальным решением будет рассмотреть приложения с сохранением состояния, которые уже умеют работать в кластере, например Kafka
\Define{Kafka}{blablabla123}
в роли брокера сообщений.

В одной из перечисленных выше статей поднималась проблема мониторинга большого количества сущностей в архитектуре Kubernetes. Есть статья~\cite{habr:flant:k8s-mon}, которая освещает именно эти моменты. Это также обзор с доклада, в этом случае с RootConf 2018. В его начале рассказывается о ключевых аспектах мониторинга, например, что спидометр показывает скорость и усреднение его редких показателей будут сильно расходится с одометром. Также освещены специфики мониторинга в Kubernetes, основной из которых является совершенно другой масштаб того, что нужно мониторить и с какой скоростью. В статье утверждается, что ключевой выбор для такого мониторинга -- Prometheus. Далее уже речь заходит об архитектуре самого Prometheus, и о том, какие метрики стоит собирать в Kubernetes. Для наглядного отображения всех агрегированных метрик используется такой инструмент как Grafana.

Кроме вышеперечисленных статей, начинающему стоит обратить внимание на пример или инструкцию развертывания простого проекта в Kubernetes с помощью GitLab CI/CD~\cite{habr:flant:k8s-helm-gitlab-sample}. Если предварительно посетить и изучить курсы по GitLab, Docker, Kubernetes и Helm, то данный материал будет предельно понятен и поможет начать создание шаблонов для развертывания продуктов в инфраструктуре.

Еще одна проблема -- накопление и, как следствие, необходимость умной очистки неиспользуемых Docker Images, освещается в статье от вышеупомянутой компании Flant~\cite{habr:flant:docker-stackoverflow}. Есть два решения данной проблемы: либо использовать фиксированное количество тегов для Docker Images, либо же каким-то образом очищать Docker Images. Во втором случае необходимо выбрать критерии актуальности образа, что также освещено в статье. Ключевой замысел реализован в подборе оптимального количества последних сохраняемых образов для каждой ветки Git-репозитория.

Выводы по рассмотренным источникам приведены в разделе \ref{sec:tools}.
